\section*{Methods}
\label{sec:methods}

\subsection*{Data}

We assume data are collected in the following manner.  Traps are dispersed, for $T$ time periods, throughout the habitat of the predator and prey of interest.  Prey species, indexed by $s \in \{1, \ldots, S \}$, are collected in the traps and counted at each time period.  We assume these counts represent the number of prey species $s$ the predator will encounter on average during time period $t \in \{1, \ldots, T\}$.  Hence, the counts of prey species $s$ caught during time period $t$ are hypothesized to be independent draws from a Poisson distribution with rate parameter $\gamma_{st}$.  We further assume that the number of prey species found in the gut of the trapped predators, also follows a Poisson distribution with rate $\lambda_{st}$.  Here, the parameter $\lambda_{st}$ represents the rate at which the predator ate the encountered prey species $s$ during time period $t$.  We then make statistical claims about the relatives rates that the $S$ prey species are eaten to the rate at which they are encountered over $T$ time periods.  

Our data collection scheme assumes the following: $1)$ traps independently catch prey that are encountered at a constant rate, $2)$ all traps are equally likely to catch the prey species of interest, $3)$ predators eat, at a constant rate, prey species independently, $4)$ predators eat indepedent of each other.  Though there exists a non-zero chance that these assumptions are broken, we feel that slight deviations from them will not overtly effect the estimates.  

Let $X_{jst} \iid \mathcal{P}(\lambda_{st})$ denote the number of prey species $s$ that predator $j$ ate during occurrence $t$ where $j \in \{1, \ldots, J_t\}$.  Let $Y_{ist} \iid \mathcal{P}(\gamma_{st})$ denote the number of prey species $s$ found in trap $i$ during occurrence $t$, $i \in \{1, \ldots, I_t\}$.  We make formal statistical statements about the relative magnitudes of the parameters $\boldsymbol{\lambda}$ and $\boldsymbol{\gamma}$.  

\begin{figure}
  \centering
  \begin{tabular}{rrcc}
    & & \multicolumn{2}{c}{Time} \\
    & & constant & varies \\
    \cline{3-4}
    \multirow{2}{*}{Species} & \multicolumn{1}{r|}{constant} & \multicolumn{1}{c}{$c$} & \multicolumn{1}{|c|}{$c_t$} \\ 
    \cline{3-4}
    & \multicolumn{1}{r|}{varies} & \multicolumn{1}{c}{$c_s$} & \multicolumn{1}{|c|}{$c_{st}$} \\
    \cline{3-4}
  \end{tabular}
  \caption{The four hypotheses considered are shown by their symbolic representations, highlighting which indices are allowed to vary.  This is essentially the range of the mapping $\xi$.  }
  \label{tab:hyp}
\end{figure}

We consider five hypotheses to provide variations on the relative magnitude of $\lambda_{st}/\gamma_{st} = \xi(c_{st})$, for a mapping $\xi: c_{st} \mapsto \{1, c, c_s, c_t, c_{st}\}$.  When $\xi$ maps to a constant with respect to one of the indexing variables, $s$ or $t$, intuitive statements are realized about the predator's preferences.  Table~\ref{tab:hyp} depicts the ways in which the proportionality constants can vary, or not, for each of the four hypotheses. Figure~\ref{fig:hier} represents the order in which hypotheses can be tested against each other; how best phrase this, does the young Skywalker?

\begin{figure}
  \centering
  \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm, semithick]

    \node[state] (A)                    {$1$};
    \node[state] (B) [below of=A]       {$c$};
    \node[state] (C) [below left of=B]  {$c_s$};
    \node[state] (D) [below right of=B] {$c_t$};
    \node[state] (E) [below left of=D]  {$c_{st}$};

    \path       (A) edge node {} (B)
                (B) edge node {} (C)
                    edge node {} (D)
                (C) edge node {} (E)
                (D) edge node {} (E);
  \end{tikzpicture}
  
  \caption{Hierarchy of hypotheses.}
  \label{fig:hier}
\end{figure}

\subsection*{Fully Observed Data}

The likelihood function that allows for estimation of these parameters is as follows.  Since we assume $X_{jst} \perp Y_{ist}$ we can simply multiply the respective Poisson probability density functions together, and then form products over all $s,t$ to get the likelihood 

\begin{equation}
  \label{eq:likelihood}
  L(x_{jst}, y_{ist} |\boldsymbol{\lambda}, \boldsymbol{\gamma}) = \prod_{t = 1}^{T} \prod_{s=1}^S \left\{ \prod_{j=1}^{J_t} f_X(x_{jst}|\boldsymbol{\lambda}) \prod_{i=1}^{I_t} f_Y(y_{ist} | \boldsymbol{\gamma}) \right\}.
\end{equation}

\noindent In some cases analytic solutions for the maximum likelihood estimates are available, namely when the data are balanced $J_t = J$, $I_t = I$, and $\xi(c_{st}) = c$ or when estimating $\lambda_{st}, \gamma_{st}$ under the hypothesis represented by $c_{st}$.  In all other cases, analytic solutions are not readily available.  In such cases, we rely on the fact that the log-likelihood $l(\boldsymbol{\lambda}, \boldsymbol{\gamma}) = \log{L}$ is concave and iteratively solve 

\begin{equation*}
  \hat{c} = \frac{\sum_{s,t} X_{\cdot st}}{\sum_t J_t \sum_s \gamma_{st}}, \quad \hat{c}_t =  \frac{\sum_s X_{\cdot st}}{J_t \sum_s \gamma_{st}}, \text{ or} \quad \hat{c}_s = \frac{\sum_{t}X_{\cdot st}}{\sum_t J_t \gamma_{st}}, \text{ and } \quad \hat{\gamma}_{st} = \frac{X_{\cdot st} + Y_{\cdot st}}{J_t\xi(c_{st}) + I_t}.
\end{equation*}

\noindent where $X_{\cdot st} := \sum_{j}X_{jst}$ and $Y_{\cdot st} := \sum_{i} Y_{ist}$.  For the case $c_{st}$, the $2(S \cdot T)$ parameters are readily found via

\begin{equation*}
  \hat{\lambda}_{st} = \frac{X_{\cdot st}}{J_t} \quad \text{ and } \quad \hat{\gamma}_{st} = \frac{Y_{\cdot st}}{I_t}.
\end{equation*}

\subsection*{Unobserved Counts}

Probably need some words about why we might not observed the count data.  

When the data $X_{jst}$ are observed as a binary response, instead of count data representing the number of prey species $s$ that the predator ate in time period $t$, we can still estimates the parameters of interest $\boldsymbol{\lambda}, \boldsymbol{\gamma}$.  Because some information is observed, we can treat the counts as missing and use the EM algorithm to find the maximum likelihood estimates of the observed data likelihood.  

We denote the binary response that the predator did in fact eat at least one prey species $s$ in time period $t$ by $Z_{jst} = 1(X_{jst} > 0)$.  Now, the observed data are $Z_{jst} \iid \text{Bern}(1-\exp\{-\lambda_{st}\})$.  Using this we can find the complete data likelihood by first noting that the conditional distribution of the unobserved data $X_{jst}$ given $Z_{jst}, Y_{ist}, \boldsymbol{\lambda}, \boldsymbol{\gamma}$ is a truncated Poisson distribution

\begin{equation*}
  f_{X|Y,Z,\boldsymbol{\lambda},\boldsymbol{\gamma}}(x_{jst}) =
  \frac{\exp{\{-\lambda_{st}\}} \lambda_{st}^{x_{jst}}}{(1 - \exp{\{-\lambda_{st}\}}) x_{jst}!}1(x_{jst} > 0) \quad \text{ where } \quad \E_{[X|Y,Z]}X_{jst} = \frac{\lambda_{st} \exp{\{\lambda_{st} \}}}{\exp{\{ \lambda_{st} \}} - 1}.
\end{equation*}

\noindent From this conditional distribution we get the joint distribution of $X_{jst}, Z_{jst}$

\begin{equation*}
    f_{X,Z|\boldsymbol{\lambda}}(x_{jst},z_{jst}) = \left\{
    \begin{array}{lr}
      \exp{\{ -\lambda_{st} \}}, & x_{jst}=0 \mbox{ and } Z_{jst} = 0 \\
      \frac{\exp{\{-\lambda_{st} \}} \lambda_{st}^{x_{jst}}}{x_{jst}!}, & x_{jst} > 0 \mbox{ and } Z_{jst} = 1\\
      0 & \mbox{otherwise}
    \end{array}
  \right.
\end{equation*}

\noindent The E-step of the EM algorithm is completed by computing the expected value of the complete data log-likelihood with respect to $f_{X|Y,Z,\boldsymbol{\lambda},\boldsymbol{\gamma}}(x_{jst})$ to get

\begin{align*}
  \E l_{comp} 
  & = \E \log{f_{X,Z|\boldsymbol{\lambda}}(X_{jst},z_{jst})} + \log{f_{Y|\boldsymbol{\gamma}}(y_{ist})} \\
  & = \sum_{s=1}^S \sum_{t=1}^T \sum_{j=1}^{J_t} \E \log{f_{X,Z|\boldsymbol{\lambda}}(X_{jst},z_{jst})}
  + \sum_{s=1}^S \sum_{t=1}^T \sum_{i=1}^{I_t} \log{f_{Y|\boldsymbol{\gamma}}(y)} \\
  & = \sum_{s,t,j} \left( - \lambda_{st} 
    + z_{jst} \E X_{jst} \log{\lambda_{st}}  \right) + \sum_{s,t} \left( -I_t \gamma_{st} + Y_{\cdot st} \log{I_t \gamma_{st}} \right) + \text{const} \\
  & = \sum_{s,t} \left( -J_t \lambda_{st} + z_{\cdot st} \E X_{jst} \log{\lambda_{st}} \right) + \sum_{s,t} \left( -I_t \gamma_{st} + Y_{\cdot st} \log{I_t \gamma_{st}} \right) + \text{const}.
\end{align*}

The EM algorithm requires iteratively solving $(\boldsymbol{\lambda}^{(k+1)},\boldsymbol{\gamma}^{(k+1)}) = \argmax_{\boldsymbol{\lambda},\boldsymbol{\gamma}} \E l_{comp}$.  Though no analytic solution to this maximization exists, one idea is to iteratively solve partial derivatives of $\E l_{comp}$ set equal to zero until convergence.  In fact, as we only need find parameter values that increase the observed likelihood, we forgo fully iterating to find the maximized values of $(\boldsymbol{\lambda}, \boldsymbol{\gamma})$ and instead perform just one step uphill within each EM iteration.  This strategy is significantly less computationally intensive, thus generating a much faster generalized EM (GEM) algorithm.  

The updating steps of the EM algorithm simply replace any occurrence of $X_{\cdot st}$ with $z_{\cdot st}\E X_{jst}$.  For instance, when maximizing under the hypothesis $c_{st}$ we get $\hat{\lambda}_{st} = z_{\cdot st} \E l_{comp} / J_t$.  In the case of the hypothesis $c_t$, where $\lambda_{st} = c_t \gamma_{st}, \, \forall t$, we iterate the following functions until convergence, 

\begin{equation*}
  \hat{\gamma}_{st} = \frac{z_{\cdot st} \E X_{jst} + Y_{\cdot st}}{cJ_t + I_t} \quad \text{ and } \quad \hat{c} = \frac{ \sum_{t} z_{\cdot st} \E X_{jst}}{ J_t \sum_{s} \gamma_{st}}
\end{equation*}

\noindent updating the value of $\lambda_{st}$ before each next step. 

This GEM algorithm works well when values $\lambda_{st}$ are relatively small and zeros are common in the data $Z_{jst}$.  In this case, not too much information is lost since estimation of $\E Z_{jst}$ can be estimated well by the proportion of observed zeros.  On the other hand, if the predator consistently eats a given prey species, few to no zeros will show up in the observed data and $\E Z_{jst}$ is estimated to be nearly $1$.  The loss of information is best seen by attempting to solve for $\lambda_{st}$ in the equation $1 = \E Z_{jst} = 1 - \exp\{-\lambda_{st}\}$; essentially $\lambda_{st}$ is sent off to $+\infty$. 

\subsection*{Testing}

All hypotheses are evaluated via a likelihood ratio test (LRT), with statistic

\begin{equation*}
  \label{eq:LRT}
    \Lambda(X,Y) := -2 \log{ \frac{ \sup L(\theta_0|X,Y)}{ \sup L(\theta_1|X,Y)} },
\end{equation*}

\noindent where $\theta_0, \theta_1$ represent the parameters estimated under the null and alternative hypotheses, respectively.  It is well known that the asymptotic distribution of $\Lambda$ is a $\chi_{\rho}^2$ distribution with $\rho$ degrees of freedom.  Under the EM algorithm we use $L_{obs}(Z,Y)$ as the likelihood in the calculation of $\Lambda$.  

The degrees of freedom $\rho$ are set equal to the number of free parameters available in the stated hypotheses under question.  If we put the null hypothesis to be $H_0: \lambda_t = c_t \gamma_t, \forall t$ and contrast this against $H_1: \lambda_{st} = c_{st}\gamma_{st}$ then there are $\rho = 2(S \cdot T) - S \cdot T - T = S \cdot T - T$ degrees of freedom.  The alternative hypothesis previously stated is not in fact fit by estimating $c_{st}$, but is instead fit as if $c_{st}\gamma_{st}$ were $S \cdot T$ unique parameters independent of $\lambda_{st}$ for all $s,t$, thus providing $2(S \cdot T)$ total parameters for the hypothesis we denote by $c_{st}$.  

A set of hypotheses is determined by the p-value of the $\chi^2_{\rho}$ distribution.  We choose the common value of $\alpha = 0.05$ as our level of significance.  Hence, if $\mathbb{P}(\chi^2_{\rho} > \Lambda) < 0.05$ the null hypothesis is rejected in favor of the alternative hypothesis.  

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
